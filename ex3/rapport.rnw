\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}

\title{TMA 4300 -- Computer Intensive Statistical Methods\\
Exercise 3}
\author{Bj√∏rn Rustad, 707765}
\date{\today}

\begin{document}

\maketitle

\section*{Problem A}

We start by loading the needed libraries and data.
<<tidy=FALSE>>=
library(rpart)
library(MASS)
data(kyphosis)
@

\subsection*{(1)}
A linear discriminant model is fitted using the \texttt{lda()} function. In the formula we specify that the \texttt{Kyphosis} indicator should be predicted using the \texttt{Age}, \texttt{Number} and \texttt{Start} variables.
<<tidy=FALSE>>=
lfit <- lda(Kyphosis ~ Age + Number + Start, kyphosis)
@

The folloving function performs $k$-fold cross validation on a discriminant model, either of the linear or quadratic type.
<<tidy=FALSE>>=
# K-fold cross validation
# - da:   function returning a discriminant model (e.g. lda or qda)
# - f:    formula to be used in model
# - data: data table
# - k:    approximate number of rows in each group
cv <- function(da, f, data, k) {
  # Calculate the number of groups
  groups <- ceiling(nrow(data) / k)

  miss <- 0
  # Loop through the groups
  for (i in 1:groups) {
    # Indices of all the members of the current group
    ind  <- (i * 10 - 9):min((i * 10), nrow(data))

    # Create the *DA model using all data except the data in the group
    fit <- da(f, data[-1 * ind,]);
    
    # Predict values for the group members
    for (i in ind) {
      # Get indicator value for current index and compare with prediction
      if (with(data[i,], get(toString(f[2]))) != predict(fit, data[i,])$class[1]) {
        miss <- miss + 1
      }
    }
  }
  
  return(miss / nrow(data))
}
@

Using this function we perform 10-fold cross validation on the linear discriminant model.
<<tidy=FALSE>>=
cv(lda, Kyphosis ~ Age + Number + Start, kyphosis, 10)
@

Next, we do the same for the quadratic discriminant model.
<<tidy=FALSE>>=
cv(qda, Kyphosis ~ Age + Number + Start, kyphosis, 10)
@
We see that for this data, the linear discriminant seems to perform a bit better.

\subsection*{(2)}
To be able to use the \texttt{knn()} function we load the \texttt{class} library of R.
<<tidy=FALSE>>=
library(class)
@

The following function performs $n$-fold cross validation on the $k$-nearest neighbor classifier.
<<tidy=FALSE>>=
# K-fold cross validation for KNN
# - data:      data table
# - cl:        the actual class of the data
# - k:         number of nearest neighbors to use in the KNN classifier
# - groupsize: approximate number of rows in each group
knncv <- function(data, cl, k, groupsize) {
  # Calculate the number of groups
  groups <- ceiling(nrow(data) / groupsize)

  miss <- 0
  # Loop through the groups
  for (i in 1:groups) {
    # Indices of all the members of the current group
    ind  <- (i * 10 - 9):min((i * 10), nrow(data))

    # Obtain the knn predictions for the test set
    kfit <- knn(train = data[-1 * ind,], test = data[ind,], cl = cl[-1 * ind], k = k)

    # Count the errors
    correct = cl[ind]
    for (i in 1:length(kfit)) {
      if (kfit[i] != correct[i]) {
        miss <- miss + 1
      }
    }
  }
  
  return(miss / nrow(data))
}
@

Using this function we find what $k$ gives the best KNN classifier.
<<tidy=FALSE>>=
best <- 1
for (k in 1:60) {
  res <- knncv(kyphosis[,2:4], kyphosis[,1], k, 10)
  if (res < best) {
    kbest <- k
    best  <- res
  }
}

print(kbest)
print(best)
@

\section*{Problem B}
First, we load the given helper function and data table.
<<tidy=FALSE>>=
source('probBhelp.R')
source('probBdata.R')
@
For the data in \texttt{data3A} we have the following model
\begin{equation}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + \epsilon_t
\end{equation}

Minimizing the sum of squared residuals, and sum of absolute residuals, we obtain the minimizers $\hat{\beta}_{LS}$ and $\hat{\beta}_{LA}$. They are calculated using the provided function \texttt{ARp.beta.est()}.
<<tidy=FALSE>>=
afit <- ARp.beta.est(data3A$x, 2)
@

The vectors of the residuals obtained using these two estimators are calculated using the \texttt{ARp.resid()} function.
<<tidy=FALSE>>=
afit$LSe <- ARp.resid(data3A$x, afit$LS)
afit$LAe <- ARp.resid(data3A$x, afit$LA)
@

The following function performs the residual resampling bootstrap method on a given $\beta$ estimator. First, it picks a random starting point in the initial chain of measurements. By resampling the residuals obtained earlier, and the given $\beta$ estimator, it generates a new chain of $x$ values. From this chain we get one bootstrap estimate for the $\beta$ parameter. We repeat this to obtain a bootstrap distribution for the estimator.
<<tidy=FALSE>>=
# Bootstrap method for the ARp beta estimators
# - x:     vector with measured x values
# - eobs:  residual vector
# - beta:  vector with two beta estimators
# - nseq:  length of chain to generate
# - n:     number of bootstrap estimates to obtain
# - field: name of beta estimator to be used (LS or LA)
boot <- function(x, eobs, beta, nseq, n, field) {
  res <- matrix(data=NA, nrow=n, ncol=2)
  
  for (i in 1:n) {
    # Sample from the residuals with replacement
    e <- sample(eobs, nseq, replace=TRUE)
    
    # Find a random starting point in the chain
    x0start <- sample(1:(length(x)-1), 1)
    x0 <- x[x0start:(x0start+1)]
    
    # Create new x vector using beta estimate and resampled residuals
    x <- ARp.filter(x0, beta, e)

    # Compute bootstrap estimate
    res[i,] <- with(ARp.beta.est(x, 2), get(field))
  }
  
  return(res)
}
@

We use this function to compute a bootstrap distribution of the two $\beta$ parameters for each estimator method.
<<tidy=FALSE>>=
LSboot <- boot(data3A$x, afit$LSe, afit$LS, 100, 1500, "LS")
LAboot <- boot(data3A$x, afit$LAe, afit$LA, 100, 1500, "LA")
@

Figure \ref{fig:lsboot} is plotted as follows, and shows how the boostrap estimates of the least squares $\beta$ estimators are distributed.
<<lsboot, fig.cap='LSBOOT', cache=TRUE, eval=TRUE, echo=TRUE, fig.height=3>>=
par(mfrow=c(1,2))
hist(LSboot[,1], 40)
hist(LSboot[,2], 40)
@

Figure \ref{fig:laboot} shows the same, for the least absolute residual estimator.
<<laboot, fig.cap='LSBOOT', cache=TRUE, eval=TRUE, echo=TRUE, fig.height=3>>=
par(mfrow=c(1,2))
hist(LAboot[,1], 40)
hist(LAboot[,2], 40)
@
We can see from the histograms that the least absolute value estimator has a sharper peak, with less variance.

The variance is computed and compared
<<tidy=FALSE>>=
var(LSboot[,1])
var(LSboot[,2])
var(LAboot[,1])
var(LAboot[,2])
@
and we see that the variance is about ten times smaller for the least absolute value estimator.

For the bias, we compute the mean of the bootstrap estimates, and subtract the original $\beta$ estimate
<<tidy=FALSE>>=
mean(LSboot[,1]) - afit$LS[1]
mean(LSboot[,2]) - afit$LS[2]
mean(LAboot[,1]) - afit$LA[1]
mean(LAboot[,2]) - afit$LA[2]
@

To find a prediction interval for the next value in the chain, $x_{101}$, we propose a value of $x_{101}$ for each bootstrap estimator. Since we are interested in a prediction interval, an interval in which we are 95\% sure that the next value will fall, we have to take the residuals into consideration. For each proposed $x_{101}$ value we add a random sample from the residuals of the $\beta$ estimator. Of course, if we only wanted to give the value which we thought were most probable to be $x_{101}$ we would not add the residual.

The computation is done as follows for the least squares estimator.
<<tidy=FALSE>>=
x99  <- data3A$x[99] 
x100 <- data3A$x[100]
lsx101 <- rep(0, nrow(LSboot))
for (i in 1:nrow(LSboot)) {
  lsx101[i] <- LSboot[i, 1] * x100 + LSboot[i, 2] * x99 + sample(afit$LSe, 1)
}
@

We repeat the prediction for the least absolute residual estimator.
<<tidy=FALSE>>=
x99  <- data3A$x[99]
x100 <- data3A$x[100]
lax101 <- rep(0, nrow(LAboot))
for (i in 1:nrow(LAboot)) {
  lax101[i] <- LAboot[i, 1] * x100 + LAboot[i, 2] * x99 + sample(afit$LAe, 1)
}
@
Figure \ref{fig:x101} shows the distribution of the predicted values for both of the two different estimators.
<<x101, fig.cap='$x_{101}$', cache=TRUE, eval=TRUE, echo=TRUE, fig.height=4>>=
par(mfrow=c(1,2))
hist(lsx101, 50, main="LS estimator")
hist(lax101, 50, main="LA estimator")
@
We create two quantile-based confidence intervals using the built-in function \texttt{quantile()} of R as follows.
<<tidy=FALSE>>=
# Quantile-based confidence interval

# For the Least Squares estimator
quantile(lsx101, c(0.025, 0.975))
# For the Least Absolute residual estimator
quantile(lax101, c(0.025, 0.975))
@

\section*{Problem C}
\subsection*{(1) Boxplot}
First we load the provided data file.
<<tidy=FALSE>>=
bilirubin <- read.table("bilirubin.txt", header=T)
@
Figure \ref{fig:boxplot} shows a boxplot of the logarithm of the concentration for each person involved in the test.
<<boxplot, fig.cap='BOXPLOT', cache=TRUE, eval=TRUE, echo=TRUE, fig.height=4>>=
boxplot(log(meas) ~ pers, data=bilirubin)
@

\subsection*{(2) Linear model}

We use \texttt{lm()} to create a fit for our model
\begin{equation}
\log Y_{ij} = \beta_{i} + \epsilon_{ij}.
\end{equation}
Although the model equation does not include an intercept variable, we will let R include an intercept. If we had not included an intercept, the F-test would give that the model is much better than it actually is, since some of the intercept would be in each $\beta$ parameter.
<<tidy=FALSE>>=
# We want the intercept.
fit <- lm(log(meas) ~ pers, bilirubin)
summary(fit)
Fval <- summary(fit)$fstatistic[1]
@
The F value is stored in \texttt{Fval}.

\subsection*{(3) \emph{permTest()}}
We write a function \texttt{permTest()} which permutes the measured values in the data frame. It also fits the linear model again, and returns the F-value.
<<tidy=FALSE>>=
permTest <- function(billy) {
  billy$meas <- sample(billy$meas)
  lfit <- lm(log(meas) ~ pers, billy)
  return(summary(lfit)$fstatistic[1])
}
@

\subsection*{(4) Permutation test}
We run the function created in the last exercise 999 times to obtain 999 F-values. 
<<tidy=FALSE>>=
Fs <- rep(NA, 999)
for (i in 1:999) {
  Fs[i] <- permTest(bilirubin)
}
@

To test our original F-value obtained in the original linear model, we check how many of the bootstrapped F-values are greater than the original one. Dividing by the total number of F values, we obtain a $p$-value for how probable it is to obtain a permutation at least as extreme as the original one, by chance.
<<tidy=FALSE>>=
length(Fs[Fs > Fval]) / length(Fs)
@

\section*{Problem D}

\subsection*{(1)}
We impute the missing value exactly as described in the lectures
\begin{equation}
\begin{aligned}
\hat{\mu} &= \bar{y} \\
\hat{\alpha}_i &= \bar{y}_{i.} - \bar{y} \\
\hat{\beta}_j  &= \bar{y}_{.j} - \bar{y}.
\end{aligned}
\end{equation}
Imputing the missing value $y_{22}$ with
\begin{equation}
y_{22} = \hat{\mu} + \hat{\alpha}_2 + \hat{\beta}_2
\end{equation}
maximizes the likelihood of the model.
<<tidy=FALSE, message=FALSE>>=
require(reshape)

y <- rbind(c(5, 8, 7), c(10, NA, 12))

old <- NA

# Maybe a bit wierd this setup...
plt <- list("a1", "a2", "b1", "b2", "b3", "id")

i <- 1
while (TRUE) {
  # Mean of all the y values
  mu = mean(y, na.rm=TRUE)
  # Estimate the parameters. This is the expectation step
  plt$a1[i] <- mean(y[1,], na.rm=TRUE) - mu
  plt$a2[i] <- mean(y[2,], na.rm=TRUE) - mu
  plt$b1[i] <- mean(y[,1], na.rm=TRUE) - mu
  plt$b2[i] <- mean(y[,2], na.rm=TRUE) - mu
  plt$b3[i] <- mean(y[,3], na.rm=TRUE) - mu
  plt$id[i] <- i
  # Fill in missing value. This is the maximization step
  y[2,2]   <- mu + plt$a2[i] + plt$b2[i]
  diff     <- old - y[2,2]
  
  i <- i + 1
  old <- y[2,2]
  
  if (!is.na(diff) && abs(diff) < 1e-5) break
}

df <- data.frame(plt$id, plt$a1, plt$a2, plt$b1, plt$b2, plt$b3)
colnames(df) <- c('id', 'a1', 'a2', 'b1', 'b2', 'b3')

dfr <- melt(df, id.vars=1)

print(y[2,2])
@

\subsection*{(2)}

Figure \ref{fig:emplot} shows a plot of how the different parameters evolve.
<<emplot, fig.cap='EM', cache=TRUE, eval=TRUE, echo=TRUE, fig.height=2.5, fig.width=6, fig.align='center'>>=
require(ggplot2)
ggplot(dfr, aes(x=id, y=value, colour=variable)) + geom_line()
@

\end{document}
